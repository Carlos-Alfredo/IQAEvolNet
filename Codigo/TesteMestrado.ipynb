{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h0cICW_G98KS"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "import time\n",
    "import sys\n",
    "import torch.multiprocessing as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jb4oKsxS-BnJ"
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, 'C:/Users/carlo/Documents/GitHub/Mestrado/Codigo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KxQImS5R-EEg"
   },
   "outputs": [],
   "source": [
    "from inference import image_haze_removel\n",
    "import metric  as  metric\n",
    "import evolution as  evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2mkrd5h6UsF2"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import evolution\n",
    "importlib.reload(evolution)\n",
    "importlib.reload(torch.multiprocessing)\n",
    "if __name__ == '__main__':\n",
    "\t\tmp.set_start_method('spawn',force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TSvY-NWx-HVN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio, MultiScaleStructuralSimilarityIndexMeasure\n",
    "from inference import image_haze_removal\n",
    "# from torcheval.metrics import PeakSignalNoiseRatio\n",
    "import lightdehazeNet\n",
    "\n",
    "def dehaze_test(image_path,weight):\n",
    "\tmetrics = np.asarray([0,0])\n",
    "\tinference_time = 0\n",
    "\tmetric_time = 0\n",
    "\timage_conversion_time = 0\n",
    "\timage_load_time = 0\n",
    "\tfor input in image_path:\n",
    "\t\tstart = time.time()\n",
    "\t\thazy_input_image = Image.open(input)\n",
    "\t\tend = time.time()\n",
    "\t\timage_load_time += end-start\n",
    "\t\tstart = time.time()\n",
    "\t\tdehaze_image = image_haze_removel(hazy_input_image,weight)\n",
    "\t\tend = time.time()\n",
    "\t\tinference_time += end-start\n",
    "\t\t# torchvision.utils.save_image(dehaze_image, \"dehaze.jpg\")\n",
    "\t\tstart = time.time()\n",
    "\t\ttensor_image = dehaze_image  # Your tensor image\n",
    "\t\ttensor_image = tensor_image.cpu()  # Move tensor to CPU if it's on GPU\n",
    "\t\ttensor_image = tensor_image.squeeze(0)  # Remove the batch dimension if present\n",
    "\n",
    "\t\t# Convert the tensor to a PIL image\n",
    "\t\tto_pil = ToPILImage()\n",
    "\t\tpil_image = to_pil(tensor_image)\n",
    "\t\t# hazy_input_image.show()\n",
    "\t\t# pil_image.show()\n",
    "\n",
    "\t\thazy_input_image = np.array(hazy_input_image)\n",
    "\t\tenhanced_image = np.array(pil_image)\n",
    "\t\tend = time.time()\n",
    "\t\timage_conversion_time += end-start\n",
    "\t\tstart = time.time()\n",
    "\t\tmetrics = metrics + np.asarray([metric.PSNR(hazy_input_image,enhanced_image),metric.SSIM(hazy_input_image,enhanced_image)])\n",
    "\t\tend = time.time()\n",
    "\t\tmetric_time += end-start\n",
    "\tprint(\"Inference time:\",inference_time)\n",
    "\tprint(\"Metric time:\",metric_time)\n",
    "\tprint(\"Image conversion time\",image_conversion_time)\n",
    "\tprint(\"Image load time\",image_load_time)\n",
    "\treturn metrics/len(image_path)\n",
    "\n",
    "def dehaze_test_preload(image_path,weight):\n",
    "\tmetrics = np.asarray([0,0])\n",
    "\tld_net = lightdehazeNet.LightDehaze_Net().cuda()\n",
    "\tld_net.load_state_dict(weight)\n",
    "\tfor input in image_path:\n",
    "\t\thazy_input_image = Image.open(input)\n",
    "\t\tdehaze_image = image_haze_removal(hazy_input_image,ld_net)\n",
    "\t\t# torchvision.utils.save_image(dehaze_image, \"dehaze.jpg\")\n",
    "\t\ttensor_image = dehaze_image  # Your tensor image\n",
    "\t\ttensor_image = tensor_image.cpu()  # Move tensor to CPU if it's on GPU\n",
    "\t\ttensor_image = tensor_image.squeeze(0)  # Remove the batch dimension if present\n",
    "\n",
    "\t\t# Convert the tensor to a PIL image\n",
    "\t\tto_pil = ToPILImage()\n",
    "\t\tpil_image = to_pil(tensor_image)\n",
    "\t\t# hazy_input_image.show()\n",
    "\t\t# pil_image.show()\n",
    "\n",
    "\t\thazy_input_image = np.array(hazy_input_image)\n",
    "\t\tenhanced_image = np.array(pil_image)\n",
    "\t\tmetrics = metrics + np.asarray([metric.PSNR(hazy_input_image,enhanced_image),metric.SSIM(hazy_input_image,enhanced_image)])\n",
    "\treturn metrics/len(image_path)\n",
    "\n",
    "\n",
    "\n",
    "def dehaze_test_tf(image_path,weight):\n",
    "\tmetrics = np.asarray([0,0])\n",
    "\tfor input in image_path:\n",
    "\t\thazy_input_image = Image.open(input)\n",
    "\t\thazy_input_image_tensor = torchvision.transforms.functional.pil_to_tensor(hazy_input_image)\n",
    "\t\t#hazy_input_image_tensor = torch.as_tensor(hazy_input_image_tensor, device=torch.device('cuda'))\n",
    "\t\tdehaze_image = image_haze_removel(hazy_input_image,weight)\n",
    "\t\t# torchvision.utils.save_image(dehaze_image, \"dehaze.jpg\")\n",
    "\t\t#tf_image = torch.as_tensor(dehaze_image, device=torch.device('cuda'))\n",
    "\t\thazy_input_image_tensor = tf.transpose(tf.convert_to_tensor(hazy_input_image_tensor.numpy()))\n",
    "\t\ttf_image = tf.transpose(tf.convert_to_tensor(tf.cast(dehaze_image.cpu().squeeze(0).detach().numpy()*255, tf.uint8)))\n",
    "\t\tmetrics = metrics + np.asarray([tf.image.psnr(hazy_input_image_tensor,tf_image,255),tf.image.ssim(hazy_input_image_tensor,tf_image,255)])\n",
    "\treturn metrics/len(image_path)\n",
    "\n",
    "def dehaze_test_torch(image_path,weight):\n",
    "\tpsnr = PeakSignalNoiseRatio().to('cuda:0')\n",
    "\tssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0).to('cuda:0')\n",
    "\tmetrics = np.asarray([0,0])\n",
    "\tfor input in image_path:\n",
    "\t\thazy_input_image = Image.open(input)\n",
    "\t\thazy_input_image_tensor = ((torchvision.transforms.functional.pil_to_tensor(hazy_input_image)).cuda()).unsqueeze(0)\n",
    "\t\tdehaze_image = image_haze_removel(hazy_input_image,weight)\n",
    "\t\t# torchvision.utils.save_image(dehaze_image, \"dehaze.jpg\")\n",
    "\t\ttf_image = dehaze_image\n",
    "\t\ttensor_image = dehaze_image  # Your tensor image\n",
    "\t\ttensor_image = tensor_image.cpu()  # Move tensor to CPU if it's on GPU\n",
    "\t\ttensor_image = tensor_image.squeeze(0)  # Remove the batch dimension if present\n",
    "\n",
    "\t\t# Convert the tensor to a PIL image\n",
    "\t\tto_pil = ToPILImage()\n",
    "\t\tpil_image = to_pil(tensor_image)\n",
    "\t\t# hazy_input_image.show()\n",
    "\t\t# pil_image.show()\n",
    "\t\tmetrics = metrics + np.asarray([psnr(tf_image, hazy_input_image_tensor).cpu().detach().numpy(),ssim(tf_image, hazy_input_image_tensor).cpu().detach().numpy()])\n",
    "\treturn metrics/len(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_split(original_list,number_of_splits):\n",
    "    sub_list = []\n",
    "    split = []\n",
    "    number_of_elements = len(original_list)\n",
    "    split_size = int(number_of_elements/number_of_splits)\n",
    "    extra_elements = number_of_elements%number_of_splits\n",
    "    last_element = 0\n",
    "    for i in range(0,number_of_splits):\n",
    "        if i < extra_elements:\n",
    "            sub_list.append(original_list[last_element:last_element+split_size+1])\n",
    "            last_element = last_element+split_size+1\n",
    "        else:\n",
    "            sub_list.append(original_list[last_element:last_element+split_size])\n",
    "            last_element = last_element+split_size\n",
    "    return sub_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/img_teste/\"\n",
    "\n",
    "weight = torch.load('C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/trained_weights/trained_LDNet.pth')\n",
    "\n",
    "path_list = []\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "\n",
    "path_split = list_split(path_list,4)\n",
    "\n",
    "argument_splited=[]\n",
    "for i in range(0,len(path_split)):\n",
    "    argument_splited.append((path_split[i],weight))\n",
    "\n",
    "pool = mp.Pool(processes=4)\n",
    "start = time.time()\n",
    "results = pool.map(dehaze_test, argument_splited)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sW1yUBug-KGb"
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/img_teste/\"\n",
    "\n",
    "img = path+ \"047.jpg\"\n",
    "\n",
    "weight = torch.load('C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/trained_weights/trained_LDNet.pth')\n",
    "# weight_cpu = torch.load('C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/trained_weights/trained_LDNet.pth',map_location='cpu')\n",
    "\n",
    "device1 = [\tpath+ \"017.jpg\",path+ \"018.jpg\",path+ \"019.jpg\",path+ \"020.jpg\",path+ \"021.jpg\",path+ \"022.jpg\",path+ \"023.jpg\",path+ \"024.jpg\"]\n",
    "device2 = [\tpath+ \"001.jpg\", path+ \"002.jpg\",path+ \"003.jpg\",path+ \"004.jpg\",path+ \"005.jpg\",path+ \"006.jpg\",path+ \"007.jpg\",path+ \"008.jpg\",\n",
    "\t\t\tpath+ \"009.jpg\",path+ \"010.jpg\",path+ \"011.jpg\",path+ \"012.jpg\",path+ \"013.jpg\",path+ \"014.jpg\",path+ \"015.jpg\",path+ \"016.jpg\"]\n",
    "device3 = [\tpath+ \"025.jpg\", path+ \"026.jpg\",path+ \"027.jpg\",path+ \"028.jpg\",path+ \"029.jpg\",path+ \"030.jpg\"]\n",
    "\n",
    "device = []\n",
    "for file in os.listdir(path):\n",
    "\tdevice.append(path+file)\n",
    "\n",
    "evolutionProcess = evolution.EvolutionaryProcess(weight,16,4,0.01,5,device3)\n",
    "\n",
    "optimized_weight = evolutionProcess.return_apex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFTKwSis-shv",
    "outputId": "f103e7d5-8720-4179-dc46-8bbfb695f3f1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Round  0\n",
      "[[[<PIL.Image.Image image mode=RGB size=778x775 at 0x1C18B5182B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=778x775 at 0x1C18B52CDF0>], [<PIL.Image.Image image mode=RGB size=987x1200 at 0x1C1F4BB1C10>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=987x1200 at 0x1C18B563C70>]], [[<PIL.Image.Image image mode=RGB size=773x1200 at 0x1C18B518820>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=773x1200 at 0x1C18B563CD0>], [<PIL.Image.Image image mode=RGB size=880x1070 at 0x1C18B518AC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=880x1070 at 0x1C18B563D60>]], [[<PIL.Image.Image image mode=RGB size=873x1125 at 0x1C18B518AF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=873x1125 at 0x1C18B563D90>]], [[<PIL.Image.Image image mode=RGB size=1024x842 at 0x1C18B518D30>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x842 at 0x1C18B563DF0>]]]\n",
      "<class 'list'>\n",
      "Metric calculation time:  3.988997220993042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c38d4bae7be3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevolutionProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Mestrado\\Codigo\\evolution.py\u001b[0m in \u001b[0;36mevolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-> Round \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                 \u001b[0mfitness_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m                 \u001b[0mfitness_score_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_fitness_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mpopulation_ranking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_score_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Mestrado\\Codigo\\evolution.py\u001b[0m in \u001b[0;36mfitness_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m                         \u001b[0mfitness_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                         \u001b[0mfitness_measure_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_measure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                         \u001b[0mfitness_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_measure_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3370\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3372\u001b[1;33m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[0;32m   3373\u001b[0m                           out=out, **kwargs)\n\u001b[0;32m   3374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evolutionProcess.evolve()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.51152623  0.55359644]\n",
      "23.70296597480774\n",
      "Inference time: 2.169631242752075\n",
      "Metric time: 10.881061315536499\n",
      "Image conversion time 6.598401784896851\n",
      "Image load time 0.034940481185913086\n",
      "[12.51152623  0.55359644]\n",
      "19.68700075149536\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(metric)\n",
    "\n",
    "path = \"C:/Users/carlo/Documents/GitHub/ProjetoPecem/imagens_nao_rotuladas/all/\"\n",
    "\n",
    "weight = torch.load('C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/trained_weights/trained_LDNet.pth')\n",
    "\n",
    "path_list = []\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "\n",
    "# path_list = [path+\"Cam 077.3 - 11.10.2022 - 04h.jpg\"]\n",
    "\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/img_teste/\"\n",
    "\n",
    "path_list = []\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "\n",
    "#path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/indoor/gt/\"\n",
    "#for file in os.listdir(path):\n",
    "#\tpath_list.append(path+file)\n",
    "#path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/indoor/hazy/\"\n",
    "#for file in os.listdir(path):\n",
    "#\tpath_list.append(path+file)\n",
    "#path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/outdoor/gt/\"\n",
    "#for file in os.listdir(path):\n",
    "#\tpath_list.append(path+file)\n",
    "#path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/outdoor/hazy/\"\n",
    "#for file in os.listdir(path):\n",
    "#\tpath_list.append(path+file)\n",
    "start = time.time()\n",
    "print(dehaze_test_preload(path_list,weight))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "start = time.time()\n",
    "print(dehaze_test(path_list,weight))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.93990001  0.85560125]\n"
     ]
    }
   ],
   "source": [
    "from enhancement_algorithms import UM,HEF,CLAHE\n",
    "importlib.reload(metric)\n",
    "import cv2\n",
    "\n",
    "def dehaze_test_CLAHE(image_path):\n",
    "\tmetrics = np.asarray([0,0])\n",
    "\tfor image_input in image_path:\n",
    "\t\thazy_input_image = cv2.imread(image_input,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\t\tdehaze_image = CLAHE(hazy_input_image,2,8)\n",
    "\n",
    "\t\tmetrics = metrics + np.asarray([metric.PSNR(hazy_input_image,dehaze_image),metric.SSIM(hazy_input_image,dehaze_image)])\n",
    "\treturn metrics/len(image_path)\n",
    "\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/img_teste/\"\n",
    "# path = \"C:/Users/carlo/Documents/GitHub/ProjetoPecem/imagens_nao_rotuladas/all/\"\n",
    "\n",
    "weight = torch.load('C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/trained_weights/trained_LDNet.pth')\n",
    "\n",
    "path_list = []\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "\n",
    "device = [  path+ \"017.jpg\",path+ \"018.jpg\",path+ \"019.jpg\",path+ \"020.jpg\",path+ \"021.jpg\",path+ \"022.jpg\",\n",
    "            path+ \"023.jpg\",path+ \"024.jpg\",path+ \"001.jpg\", path+ \"002.jpg\",path+ \"003.jpg\",path+ \"004.jpg\",\n",
    "            path+ \"005.jpg\",path+ \"006.jpg\",path+ \"007.jpg\",path+ \"008.jpg\",path+ \"009.jpg\",path+ \"010.jpg\",\n",
    "            path+ \"011.jpg\",path+ \"012.jpg\",path+ \"013.jpg\",path+ \"014.jpg\",path+ \"015.jpg\",path+ \"016.jpg\",\n",
    "            path+ \"025.jpg\", path+ \"026.jpg\",path+ \"027.jpg\",path+ \"028.jpg\",path+ \"029.jpg\",path+ \"030.jpg\"]\n",
    "    \n",
    "# path_list = [path+\"Cam 077.3 - 11.10.2022 - 04h.jpg\"]\n",
    "path_list = []\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/indoor/gt/\"\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/indoor/hazy/\"\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/outdoor/gt/\"\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "path = \"C:/Users/carlo/Documents/GitHub/Mestrado/Codigo/SOTS/outdoor/hazy/\"\n",
    "for file in os.listdir(path):\n",
    "\tpath_list.append(path+file)\n",
    "\n",
    "print(dehaze_test_CLAHE(path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
