{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29579,"status":"ok","timestamp":1708271466134,"user":{"displayName":"carlos alfredo","userId":"13899691815169459528"},"user_tz":180},"id":"UcfJU0KlvLa4","outputId":"3961d8e8-e233-4128-f9e3-379f01c63c2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import sys\n","import torch\n","\n","drive.mount('/content/drive')\n","\n","sys.path.insert(1,'/content/drive/My Drive/Mestrado/Codigo/Imports/')"]},{"cell_type":"markdown","source":["The cell below is used for the pre-training of the model using one of the 5 enhancement algorithm: HEF, UM, CLAHE, TCDHE e ATACE.\n","\n","Select the appropriate dataset folder with the 4 sub-folders: Viral Pneumonia, Normal, Lung Opacity and COVID.\n","\n","The epoch and learning rate used were 100 and 0.0001, respectively."],"metadata":{"id":"f7iKLhgJpXYg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbirLOYYtWOZ"},"outputs":[],"source":["import numpy as np\n","import cv2\n","import lightdehazeNet\n","import lightdehazeTestnet\n","from pretrain import pretrain\n","from data_loader import folder_data_loader\n","from enhancement_algorithms import HEF,UM,CLAHE,TCDHE,xrayjapan\n","\n","enhancement_method_list = ['hef','um','clahe','tcdhe','atace']\n","\n","enhancement_method = enhancement_method_list[] # Choose the enhancement method to be used for pre-training\n","\n","model = 'lightdehazeTestnet'\n","\n","dataset = \"TestCXR\"\n","\n","dataset_folder = #\"/content/drive/My Drive/Mestrado/Codigo/Datasets/\"+dataset+'/'\n","\n","file_folder = [dataset_folder+\"images/Viral Pneumonia/\",\n","              dataset_folder+\"images/Normal/\",\n","              dataset_folder+\"images/Lung_Opacity/\",\n","              dataset_folder+\"images/COVID/\"]\n","\n","epoch = 100\n","\n","learning_rate = 0.0001\n","\n","data_loader = folder_data_loader(class_path_list=file_folder, img_size=(224,224), batch_size=1024, train_ratio = 0.5)\n","\n","torch.cuda.empty_cache()\n","\n","clipLimit = 2.0\n","\n","raio = 8\n","\n","clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(raio,raio))\n","\n","paired_dataset = []\n","\n","dataset_train = ((data_loader.get_item(index=0, mode='train'))[0]*255).astype(np.uint8)\n","\n","for i in range(0,dataset_train.shape[0]):\n","    gray_image = dataset_train[i]\n","    #enhanced_image = clahe.apply(gray_image)\n","    if enhancement_method == 'clahe':\n","      enhanced_image = CLAHE(gray_image,2.0,8)\n","    if enhancement_method == 'um':\n","      enhanced_image = UM(gray_image,5,2)\n","    elif enhancement_method == 'hef':\n","      enhanced_image = HEF(gray_image,20)\n","    elif enhancement_method == 'xrayjapan':\n","      enhanced_image = xrayjapan(gray_image)\n","    elif enhancement_method == 'tcdhe':\n","      try:\n","        enhanced_image = TCDHE(gray_image)\n","      except:\n","        enhanced_image = grayimage\n","        print(\"Error on the TCDHE!\")\n","    paired_dataset.append([gray_image,enhanced_image])\n","\n","del dataset_train\n","\n","paired_dataset = (np.asarray(paired_dataset)/255.0).astype(np.single)\n","print(paired_dataset.shape)\n","\n","np.random.shuffle(paired_dataset)\n","\n","training_size = int(paired_dataset.shape[0]*0.8)\n","\n","print(training_size)\n","\n","training_dataset = paired_dataset[0:training_size]\n","\n","validation_dataset = paired_dataset[training_size:paired_dataset.shape[0]]\n","\n","del paired_dataset\n","\n","model = 'lightdehazeTestnet'\n","\n","import importlib\n","importlib.reload(lightdehazeTestnet)\n","\n","if model == \"lightdehazeNet\":\n","  ld_net = lightdehazeNet.LightDehaze_Net().cuda()\n","  folder=dataset_folder+'Pretrained_weights/lightdehazeNet/'\n","\n","elif model == \"lightdehazeTestnet\":\n","  ld_net = lightdehazeTestnet.LightDehaze_Testnet().cuda()\n","  folder=dataset_folder+'Pretrained_weights/lightdehazeTestnet/'\n","\n","folder = folder+enhancement_method+'/'\n","\n","weight = pretrain(ld_net,training_dataset,validation_dataset,epoch,learning_rate,folder)\n","\n","print(\"Step Done!\")\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","source":["The cell bellow installs the pyiqa module necessary for the running of the algorithm."],"metadata":{"id":"2Nrjq9DPrfKM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"naTII-xlvNk2"},"outputs":[],"source":["pip install pyiqa"]},{"cell_type":"markdown","source":["The code below prepares the dataloader object."],"metadata":{"id":"_9bX4dNGrmDi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y01JfczjvP8B"},"outputs":[],"source":["from data_loader import folder_data_loader\n","import numpy as np\n","import cv2\n","import lightdehazeNet\n","import lightdehazeTestnet\n","from pretrain import pretrain\n","import IQAEvolNet\n","import time\n","\n","dataset = \"TestCXR\"\n","\n","dataset_folder = \"/content/drive/My Drive/Mestrado/Codigo/Datasets/\"+dataset+'/'\n","\n","file_folder = [dataset_folder+\"images/Viral Pneumonia/\",\n","               dataset_folder+\"images/Normal/\",\n","               dataset_folder+\"images/Lung_Opacity/\",\n","               dataset_folder+\"images/COVID/\"]\n","\n","data_loader = folder_data_loader(class_path_list=file_folder, img_size=(224,224), batch_size=32, train_ratio = 0.8, dataset_size_scaling = 0.1)"]},{"cell_type":"markdown","source":["The cell bellow runs the evolutionary algorithm."],"metadata":{"id":"zJ6NtP9Rrvpx"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-o8HVqOuSy-J"},"outputs":[],"source":["model = 'lightdehazeTestnet' #Chosen model\n","\n","metric = 'complete' # Metric folder chosen, in all cases use complete\n","\n","test = '11' # Weight set to be used\n","\n","if model == \"lightdehazeNet\":\n","  ld_net = lightdehazeNet.LightDehaze_Net().cuda()\n","  folder=dataset_folder+'Pretrained_weights/lightdehazeNet/'\n","\n","elif model == \"lightdehazeTestnet\":\n","  ld_net = lightdehazeTestnet.LightDehaze_Testnet().cuda()\n","  folder=dataset_folder+'Pretrained_weights/lightdehazeTestnet/'\n","\n","starting_population = []\n","\n","for enhancement_method in ['um','hef','clahe','xrayjapan','tcdhe']:\n","  for i in range(0,5):\n","    starting_population.append(torch.load(\"/content/drive/My Drive/Mestrado/Codigo/Datasets/\"+dataset+\"/Pretrained_weights/lightdehazeTestnet/\"+enhancement_method+\"/Epoch\"+str(99-10*i)+'.pth'))\n","\n","population_size = 25\n","\n","chance_of_mutation = 0.1\n","\n","noise_intensity = 1\n","\n","noise_decay = 0.5\n","\n","batch_size_per_round = 32\n","\n","apex_threshold = 1.00001\n","\n","if test == '1':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=1,ssim=1)\n","if test == '2':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=1,ssim=2)\n","if test == '3':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=2,ssim=1)\n","if test == '4':\n","  metric_declaration = evolution.MetricDeclaration(niqe=2,entropy=1,ssim=1)\n","if test == '5':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=4,ssim=1)\n","if test == '6':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=4,ssim=2)\n","if test == '7':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=8,ssim=1)\n","if test == '8':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=16,ssim=1)\n","if test == '9':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=0,ssim=0)\n","if test == '10':\n","  metric_declaration = evolution.MetricDeclaration(niqe=0,entropy=1,ssim=0)\n","if test == '11':\n","  metric_declaration = evolution.MetricDeclaration(niqe=1,entropy=1,ssim=0)\n","\n","evolutionProcess = IQAEvolNet.IQAEvolNet(ld_net, starting_population[0:population_size],chance_of_mutation,\n","\t\tnoise_intensity,noise_decay,data_loader,batch_size_per_round,\n","\t\tapex_threshold,metric_declaration)\n","\n","chronometer = []\n","train_results = []\n","validation_results = []\n","\n","for i in range(0,50):\n","  print('==================================================')\n","  print(\"Epoch \",i+1)\n","  start = time.time()\n","  evolutionProcess.evolve()\n","  end = time.time()\n","  chronometer.append(end-start)\n","  print(\"Epochs without improvement: \",evolutionProcess.epochs_without_improvement)\n","  x_val,y_val = evolutionProcess.validation_score()\n","  x_train = evolutionProcess.apex_score\n","  y_train = evolutionProcess.apex_fitness_measure_avg\n","  print(np.append(x_train,y_train))\n","  print(np.append(x_val,y_val))\n","  print('==================================================')\n","  train_results.append(np.append(x_train,y_train))\n","  validation_results.append(np.append(x_val,y_val))\n","  if evolutionProcess.epochs_without_improvement == 3:\n","    break\n","\n","for i,apex in enumerate(evolutionProcess.return_apex_timeline()):\n","  torch.save(apex,\"/content/drive/My Drive/Mestrado/Codigo/Enhancement/\"+dataset+'/'+metric+'/'+test+'/Generation'+str(i)+'.pth')\n","\n","torch.save(evolutionProcess.apex,\"/content/drive/My Drive/Mestrado/Codigo/Enhancement/\"+dataset+'/'+metric+'/'+test+'/apex.pth')\n","\n","np.save(\"/content/drive/My Drive/Mestrado/Codigo/Enhancement/\"+dataset+'/'+metric+'/'+test+'/chronometer',np.asarray(chronometer))\n","\n","np.save(\"/content/drive/My Drive/Mestrado/Codigo/Enhancement/\"+dataset+'/'+metric+'/'+test+'/train_results',np.asarray(train_results))\n","\n","np.save(\"/content/drive/My Drive/Mestrado/Codigo/Enhancement/\"+dataset+'/'+metric+'/'+test+'/validation_results',np.asarray(validation_results))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyM0qZW4zayDZi6mwlJ86hP6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}